base_model_path: "meta-llama/Llama-3.1-8B-Instruct"

# Tokenizer to use (usually the same as the base model)
tokenizer_path: "meta-llama/Llama-3.1-8B-Instruct"
adapter_path: "data/output/models/llama_8b_sft_clinical_codes_v1/"

# Dataset for evaluation
test_dataset_path: "data/input/test_dataset.jsonl"

# Output file for predictions
predictions_output_path: "data/output/models/llama_8b_sft_clinical_codes_v1/test_predictions.json"

# Inference parameters
generation_params:
  max_new_tokens: 100
  do_sample: true
  temperature: 0.7
  top_p: 0.9

# Prompt template for formatting test examples for the model
# The "{instruction}" and "{input}" will be replaced by data from the test file.
prompt_template: |
  ### Instruction:
  {instruction}

  ### Input:
  {input}

  ### Response:
